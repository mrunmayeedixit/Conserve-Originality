Abstract—Extreme learning machine (ELM) has attracted atten-tions in pattern recognition field due to its remarkable advantages such as fast operation, straightforward solution, and strong gener-alization. However, the performance of ELM for high-dimensional data, such as hyperspectral image, is still an open problem. There-fore, in this paper, we introduce ELM for hyperspectral image classification. Furthermore, in order to overcome the drawbacks of ELM caused by the randomness of input weights and bias, two new algorithms of ensemble extreme learning machines (Bagging-based and AdaBoost-based ELMs) are proposed for the classification task. In order to illustrate the performance of the proposed algorithms, support vector machines (SVMs) are used for evaluation and comparison. Experimental results with real hyperspectral images collected by reflective optics spectrographic image system (ROSIS) and airborne visible/infrared imaging spectrometer (AVIRIS) in-dicate that the proposed ensemble algorithms produce excellent classification performance in different scenarios with respect to spectral and spectral–spatial feature sets.Index Terms—Bagging-based ensemble extreme learning machines (BagELMs), boostELMs, classification, ensemble extreme learning machines (   ), ensemble learning (EL), extreme learning machine (ELM), hyperspectral remote sensing. INTRODUCTION HYPERSPECTRAL imaging sensors are capable of capturing fine and abundant spectral information in hundredths of continuous narrow spectral bands. The interests for image processing techniques and wide range of applications of hyperspectral images have been greatly increased over the last decades. One of the most important processing tasks is classifi-cation, by which land-cover thematic maps are generated [1]. However, there are several important challenges when perform-ing hyperspectral image classification. For instances, unbalance between the limited training samples and high dimensionality,Manuscript received June 28, 2013; revised December 12, 2013; accepted January 15, 2014. Date of publication February 04, 2014; date of current version April 18, 2014. This paper is supported in part by Jiangsu Provincial Natural Science Foundation under Grant BK2010182, in part by the Natural Science Foundation of China under Grant 41171323, and in part by the Priority Academic Program Development of Jiangsu Higher Education Institutions (PAPD).(Corresponding author. P. Du.) A. Samat, P. Du, and L. Cheng are with the Key Laboratory for Satellite Mapping Technology and Applications of State Administration of Surveying, Mapping, and Geoinformation of China, Nanjing University, Nanjing 210023, China (e-mail: alim.smt@gmail.com; dupjrs@gmail.com; lcheng.geo@gmail. com). presence of mixed pixels, integrating the spatial and spectral information to take advantage of the complementarities, quite complex geometry, and very high computational complexity of some of the classifiers [2]. To overcome these challenges and perform hyperspectral image classification effectively, several machine learning techniques such as artificial neural networks (ANNs) [3], support vector machine (SVM) [4]–[8], multinomial logistic regression [9], active learning (AL) [10, 11], semi-supervised learning (SSL) [12], manifold learning [13], etc., and other methods like hyperspectral unmixing via sparse repre-sentation [14], morphological profiles [15], [16], and partitional clustering techniques [17] have been popularly investigated in recent years as well. However, due to the high dimensionality and complexity of hyperspectral image in spectral domain, finding optimal parameters for parametric supervised algorithms is always time-consuming and difficult. It is still a critical problem to achieve high-performance classification with fast speed and high efficiency.Recently, extreme learning machine (ELM), a single-hidden layer feedforward neural network (SLFN), has been proposed as one of the successful machine learning approaches for pattern classification [18]–[22]. ELM has been considered as a promis-ing learning algorithm in contrast with other algorithms such as BP (backward propagation) neural networks and SVM because an ELM has the following advantages: 1) it is computationally efficient; 2) it has similar even higher generalization performance in comparison with BP and SVM; 3) all piecewise continual functions can be used as activation functions such as sigmoid function, triangular basis function, radial basis function, etc.; and no extra parameters need to be tuned except the predefined network architecture. There are already plenty of works from different fields showing the capabilities of ELM in fast and accurate pattern classification [20]–[22]. However, ELM also has some drawbacks. For instance, the randomness of input weights and bias can result in ill-posed problems, which lead to low performance or even no solution. Therefore, an ELM can work as a weak learner in ensemble learning (EL), similar to other weak learners such as neural networks and decision tree [23]. According to the principles and advantages of EL in which weak learners can be combined to work as a strong learner, the shortcomings of ELM might be overcome by using ensemble strategies.In this work, performance of ELM for high-dimensional hyperspectral remote sensing image is exploited as the part of investigating capability of ELM on high-dimensional data [18]. The main contributions of this paper consist of twofolds. First, the effectiveness of ELM for hyperspectral remote sensing image classification is investigated. Second, two novel ELM algorithms are developed based on EL strategy, namely in order to control the defects of ELM, Bagging-based, and AdaBoost-based versions of ELM. The remainder of this paper is organized as follows. Section II introduces ELM and proposes Bagging- and AdaBoost-based ELM algorithms for ensemble extreme learning machines used for hyperspectral image classification. In Section III, the proposed     are evaluated by experiments with real hyperspectral data. Finally, Section IV concludes with some remarks.METHODS A)Extreme Learning Machine (ELM) In recent years, ANNs have been widely used in applications involving classification or function approximation. However, its main drawback is that the learning speed of ANNs is much slower than the actual need, which has been a major bottleneck for practical applications. To overcome this drawback, many researchers have explored approximation capability of feedfor-ward neural networks, especially in a finite training set from a mathematical point of view. As one of the most important achievements of these works, a novel learning algorithm, ELM, was proposed recently for SLFNs [18], [24]–[27]. columns set by 0, and P is the number of classes. Then, the model of single layer hidden layer	neural	network having   hidden neurons and an activation function  can be expressed as where   and   represents the weight vectors from inputs to hidden layer and from hidden layer to output layer, respectively,  is the bias of  th hidden neuron, is the output of the  the hidden neuron with respect to the input sample. Note that (1) can be rewritten in a compact form as where  is the hidden layer output matrix of SLFNs and is the output weight matrix. Optimal weights and bias of SLFNs can be found by using backpropagation learning algorithms, which requires the user to specify learning rate and momen-tum. However, there is no guarantee that the global minimum error can be found. Thus, the learning algorithm can have defects such as local minima and over-training. In exploration of the approximation capability of feedforward neural net-works in a finite training set, researchers found that SLFNs can reach the approximate capacity at a specified error      level with the hidden layer neurons is much less than the number of training samples. And based on the minimum norm least-squares function, the weight matrix   in (2) can be solved by where   is the Moore–Penrose generalized inverse of matrix  . Equation (3) can provide some remarkable advantages. For example, it straightforwardly gets the smallest norm of weights with the best generalization performance and involves no local minima as backpropagation learning algorithm generally has Hence, the ELM algorithm for classification problem can be summarized as follows [18]:Given represents training sets with	the vectorised target label, activation function  ( ), hidden neuron number  , and unknown instances .Training:Step 1) Randomly choosing input weight and bias superscript T represent matrix transpose operation.Step 2) Calculating the hidden layer output matrix Step 3) Calculating the output weight .Classification: Step 1) Calculating the hidden layer output matrix of new instances Step 2) Get the class label of new instances        . Compared with other feedforward neural networks, ELM has some remarkable advantages such as: 1) no need to tune the parameters in learning phase iteratively by using slow gradient-based learning algorithms, 2) better generalization performance in most cases, 3) the learning speed is extremely fast, and 4) tending to provide the solutions for local minima, over-fitting and improper learning rate straightforwardly without such trivial issues as weight decay and early stopping methods [18], [19]. However, the randomness of input layer weights and bias will inevitably impacts the robustness of ELM, even makes the model unsolvable in the worst case. Furthermore, the numbers of hidden neurons and activation functions also influence the performance of ELM, especially for complex data such as hyperspectral images. Therefore, in this work, we take advantage of the generalization capability of ELM and propose two algorithms which are based on EL, namely,    , aiming to overcome the aforementioned drawbacks. B. Ensemble Extreme Learning Machines (   ) In the context of pattern recognition field, there is no guarantee that some or one specific classifier can always achieve the best performance on every circumstance. But better predictive perfor-mance than any single classifier might be achieved through EL. This is the kernel idea of EL, also named as classifier ensemble in terms of classification task. Due to its validity in statistics, expression, and computation, EL has been widely applied in machine learning and pattern recognition fields [28]–[31]. Re-searchers have developed many EL methods such as Bagging, Boosting, Multi-boost, Random Forest, Rotation Forest, etc., and most of them have been used in remote sensing image processing with effective performance [23], [29]–[35]. The first key compo-nent of constructing an effective EL system is producing base classifiers with high diversity. In order to reach that, many techniques such as resampling, label switching, and feature space partitions have been developed. Generally, unstable classifiers (sensitive to model parameter or sensitive to training set) are primarily used as base classifier in EL system in order to reach preferred diversity. Consequently, better predictive performance than any single unstable ELM might be achieved by integrating with EL. In this work, EL is integrated with EML and two implementations of, Bagging-, and AdaBoost-based are proposed for hyperspectral image classification. 1) Bagging-Based ELMs: Similar to Bagging, Boosting is another method for improving the performance of a weak learner which is difficult for random guess [23], [38]. Adaboost (adaptive boosting), which focuses on patterns that are harder to classify, is a popular ensemble algo-rithm [34]. Via resampling, Adaboost can provide the most informative training set by changing their distribution probabili-ty dynamically for each consecutive learner. According to the concept of margins comes from statistical learning theory, maximizing the margins (“boosting the margins”) by providing the most informative training set will lead to a smaller upper bound on the testing error [39]. Therefore, an AdaBoost-based approach for ensemble extreme learning (BoostEMLs) is pro-posed in Algorithm 2. As BagELMs, BoostEMLs train ELM on independently sampled training sets based on distribution prob-ability to get different output layer weights, and then compute the final label of unknown instances.EXPERIMENTS AND ANALYSIS Data ROSIS Pavia University Hyperspectral Image: Reflective optics spectrographic image system (ROSIS) Pavia University hyperspectral image was acquired with ROSIS optical sensor which provides 115 bands with a spectral range coverage ranging from 0.43 to    µ . The spatial resolution is 1.3 m. The image shown in Fig. 1(a) was captured over the Engineering School,University of Pavia, Pavia, Italy. It has       pixels with 103 spectral channels and 9 classes were considered [Fig. 1(b) and Table I].Kennedy Space Center (KSC) Hyperspectral Image: KSC hyperspectral image shown in Fig. 1(e) was acquired with airborne visible/infrared imaging spectrometer (AVIRIS) sensor over the Kennedy Space Center (KSC), Florida, USA, on March 23, 1996. This image has 224 bands from 400 to 2500 nm and the spatial resolution is 18 m. After removing water absorption and low signal-to-noise (SNR) bands, it has  pixels with 176 bands. Thirteen classes are used for this site [Fig. 1(f) and Table I].Salinas Hyperspectral Image: Salinas scene shown in Fig. 1(c) was collected by the 224-band AVIRIS sensor over Salinas Valley, California, and is characterized by high spatial resolution (3.7-m pixels). The area covered comprises 512 lines by 217 samples. After discarding the 20 water absorption bands (108–112, 154–167, 224), 204 bands are used for classification. There are 16 classes in the ground truth image [Fig. 1(d) and Table I].Experimental Results and Analyses In order to perform spectral–spatial classification, spatial features are extracted via extended morphological profiles(EMPs) [15], [16], [40] using the all spectral bands and the first 10 principal components (PCs). Then 309 and 30 profiles for the ROSIS Pavia University data, 528 and 30 profiles for the KSC data, 612 and 30 profiles for Salinas data were obtained, respec-tively. Finally, 10-fold cross-validation technique was used to where the searching range for the numbers of neurons in hidden layer of ELM and BagELMs is from 10 to 500 neurons while the range set is from 20 to 120 for BoostELMs. Evaluation of the Computational Efficiency: In the first set of experiments, the computational efficiencies of the proposed approaches were evaluated in comparison with SVM, as SVM has been proven to be a powerful tool for supervised classification problems and shown good performance [4]–[8]. Fig. 2 shows the computational costs of SVM, ELM, BagELMs, and BoostELMs for the original data and the EMPs. It is noticeable that, the ELM algorithms are much faster than SVM for the step of parameters searching. However, in general ELM methods need more time for training and classification. Nevertheless, for the whole process, ELM methods are much faster than SVM. In order to show that the proposed methods are very efficient from computational viewpoint, Tables II–IV pres-ent the time of each algorithm. Similar observation can be obtained that ELM methods are much faster than SVM. Evaluation of the Robustness: In this subsection, the robustness of the proposed approaches is investigated. As aforementioned, ELM cannot guarantee a global solution due to different conditions, which leads to large statistical variations of the final results. Fig. 3 shows the classification accuracy by different methods on different data sets. The overall accuracy surface of SVM respect with the gamma and cost factors is used for comparison [Fig. 3(g) and (h)]. In order to increase the statistical influence, ELM, BagELMs, BoostELMs, and SVM are repeatedly operated 100 times and the average is computed. Several observations can be obtained from Fig. 3. As expected, the results obtained by ELM varied quite obviously because of the randomness of input layer weights and bias. BagELMs obtained more stable results in comparison with ELM, as the instability was reduced by Bagging. BoostELMs obtained better results when using smaller numbers of hidden neurons than ELM used. Because boosting method focuses on the more informative instances which are much smaller than the size of original training instances. Furthermore, the influence of the numbers of hidden neurons for the final performance is reduced after adding the spatial information EMPs. It should be also noticed that, according to the changing range on overall accuracy, SVM is much sensitive to the model parameters compared with ELM, BagELMs, and BoostELMs. Evaluation of Classification Accuracy: In these experiments, the classification accuracies for the proposed approaches were obtained and evaluated as shown in Tables II–IV. Notice that, SVM obtained higher accuracies than ELM, BagELMs, and BoostELM in most cases when using all spectral bands or using all spectral bands with EMPs. However, due to the high dimensionality SVM took huge time for searching the optimal parameters. Furthermore, the classification accuracies of ELM, BagELMs, and BoostELM were enhanced after including the spatial information. T IV. CONCLUSION In this paper, ELM was introduced to hyperspectral image classification, and the concept of     was proposed by combining EL with ELM in order to overcome the limitation of ELM as a weak learner. In comparison with ELM and the benchmark SVM classifier, the proposed approaches, BagELMs and BoostELM are faster than SVM, more robust than conven-tional ELM, and both of them can achieve very good classifica-tion performance. Based on this work, we also envisage the future perspectives. Performance of other differential or nondifferentiable activation functions should be an interesting point in future works. Since the BagELMs and BoostELM show the state-of-the-art classification performances, other EL methods should be investigated as well. Finally, we will extend the proposed methods to kernel methods with an attempt to improve classification performance. Machine learning is the major success factor in the ongoing digital transformation across industries. Startups and behemoths alike announce new products that will learn to perform tasks that previously only humans could do, and perform those tasks better, faster, and more intelligently. But how do they do it? What does it mean for IT developers and software engineers? Here, Panos Louridas and I present a brief overview of machine-learning technologies, with a concrete case study from code analysis. I look forward to hearing from both readers and prospective column authors. —Christof Ebert Although there are fewer machine-­ learning libraries for Python than there are for R, many programmers find working with Python easier. They might already know the lan-guage or find it easier to learn than R. They also find Python convenient for preprocessing data: reading it from various sources, cleaning it, and bringing it to the required for-mats. For visualization, Python re-lies on matplotlib. You can do pretty much everything on matplotlib, but you might discover you have to put in some effort. The seaborn library is built on top of it, letting you pro-duce elegant visualizations with little code.In general, R and Python work when the dataset fits in the com- puters main memory. If that’s not possible, you must use a distributed platform. The most well-known is Hadoop, but Hadoop isnt the most convenient for machine learning. Making even simple algorithms run on it can be a struggle. So, many people prefer to work at the higher level of abstraction that Spark offers. Spark leverages Ha-doop but looks like a scripting envi-ronment. You can interact with it us-ing Scala, Java, Python, or R. Spark has a machine-learning library that implements key algorithms, so for many purposes you dont need to im-plement anything yourself.H2O is a relatively newer entrant in the machine-learning scene. Its a platform for descriptive and predic-tive analytics that uses Hadoop and Spark; you can use it with R and Py-thon. It implements supervised- and unsupervised-learning algorithms and a Web interface through which you can organize your workflow. A promising development is the Julia programming language for technical computing, which aims at top performance. Because Julia is new, it doesnt have nearly as many libraries as Python or R. Yet, thanks to its impressive speed, its popularity might grow. Strong commercial players in-clude Matlab and SAS, which both have a distinguished history. Matlab has long offered solid tools for nu-merical computation, to which it has added machine-learning algorithms and implementations. For engineers familiar with Matlab, it might be a natural fit. SAS is a software suite for advanced statistical analysis; it also has added machine-learning ca-pabilities and is popular for business intelligence tasks.that you can now architect them in new ways. ANNs can be used across the spectrum of machine learning: classification, regression, clustering, and dimensionality reduction.Innovations in ANN architec-tures and the availability of cheap computing resources to run ANNs has brought about the burgeoning of deep learning—using big ANNs to perform machine learning. Over the last few years, deep learning has chalked up headline-grabbing suc-cesses by beating humans in Jeop-ardy! and Go, learning to play ar-cade games, showing an uncanny capability to recognize images, per-forming automatic translation, and so on. Deep learning is particularly good at general tasks requiring the elicitation of higher-level, abstract concepts from the input data, which is what the many layers of an ANN excel at. Deep learning is usually imple-mented through matrices, so work-ing with it requires efficient matrix operations and manipulation. Usu-ally the implementations are in C or C++, but designing ANNs at that level is unwieldy. Python program-mers can use the Theano library to define ANNs, which are compiled to C code thats then compiled to ma-chine language. Recently, Google re-leased as open source its TensorFlow library for working with ANNs. You can interact with TensorFlow through a Python API. A C++ API is also available; although not as easy to use, it might give some perfor-mance benefits.Before jumping on the deep-­ learning bandwagon, keep in mind that all machine-learning approaches lie on a spectrum based on the ease of interpreting their results. For ex-ample, classification trees produce rules that classify data. By reading those rules, you can easily under-stand how a classification tree classi-fies data. ANNs dont produce any-thing their users can interpret. An ANN that classifies images doesnt produce any rules; the network itself embodies everything it has learned about image classification. machine learning on a particu-lar platform. As technologies quickly evolve, its better to focus on getting a solid grasp of the fundamentals. After all, using a machine-learning platform isnt difficult; knowing when to use a particular algorithm and how to use it well requires quite a bit of background knowledge. Here are four popular books:PANOS LOURIDAS is an associate professor teaching algorithms and software at the Athens University of Economics and Business. Hes also an active developer. Contact him at louridas@ aueb.gr.CHRISTOF EBERT is the managing director of Vector Consulting Services. He is on the IEEE Software editorial board and teaches at the University of Stuttgart and the Sorbonne in Paris. Contact him at christof.ebert@vector.com. Abstract—Semi-supervised learning has attracted a significant amount of attention in pattern recognition and machine learning. Most previous studies have focused on designing special algorithms to effectively exploit the unlabeled data in conjunction with labeled data. Our goal is to improve the classification accuracy of any given supervised learning algorithm by using the available unlabeled examples. We call this as the Semi-supervised improvement problem, to distinguish the proposed approach from the existing approaches. We design a metasemi-supervised learning algorithm that wraps around the underlying supervised algorithm and improves its performance using unlabeled data. This problem is particularly important when we need to train a supervised learning algorithm with a limited number of labeled examples and a multitude of unlabeled examples. We present a boosting framework for semi-supervised learning, termed as SemiBoost. The key advantages of the proposed semi-supervised learning approach are:4)performance improvement of any supervised learning algorithm with a multitude of unlabeled data, 2) efficient computation by the iterative boosting algorithm, and 3) exploiting both manifold and cluster assumption in training classification models. An empirical study on 16 different data sets and text categorization demonstrates that the proposed framework improves the performance of several commonly used supervised learning algorithms, given a large number of unlabeled examples. We also show that the performance of the proposed algorithm, SemiBoost, is comparable to the state-of-the-art semi-supervised learning algorithms. SEMI-SUPERVISED learning has received a significant inter-est in pattern recognition and machine learning. While semi-supervised classification is a relatively new field, the idea  of  using  unlabeled  samples  for  prediction  was conceived several decades ago. The initial work in semi-supervised learning is attributed to Scudders for his work on “self-learning” in 1965 [1]. An earlier work by Robbins and Monro [2] on sequential learning can also be viewed as related to semi-supervised learning. The key idea of semi-supervised  learning,  specifically  semi-supervised SEMI-SUPERVISED learning has received a significant inter-est in pattern recognition and machine learning. While semi-supervised classification is a relatively new field, the idea  of  using  unlabeled  samples  for  prediction  was conceived several decades ago. The initial work in semi-supervised learning is attributed to Scudders for his work on “self-learning” in 1965 [1]. An earlier work by Robbins and Monro [2] on sequential learning can also be viewed as related to semi-supervised learning. The key idea of semi-supervised  learning,  specifically  semi-supervised SDGUWE xnsdhe any important need SEMI-SUPERVISED learning has received a significant inter-est in pattern recognition and machine learning. While semi-supervised classification is a relatively new field, the idea  of  using  unlabeled  samples  for  prediction  was conceived several decades ago. The initial work in semi-supervised learning is attributed to Scudders for his work on “self-learning” in 1965 [1]. An earlier work by Robbins and Monro [2] on sequential learning can also be viewed as related to semi-supervised learning. The key idea of semi-supervised  learning,  specifically  semi-supervised classification, is to exploit both labeled and unlabeled data to learn a classification model. Enormous amount of data is being generated every day in the form of news articles, documents, images, and e-mail to name a few. Most of the generated data are uncategorized or unlabeled, thereby making  it  difficult  to  use  supervised  approaches  to automate applications like personal news filtering, e-mail spam filtering, and document and image classification. Typically, there is only a small amount of labeled data available, for example, based on which articles a user marks interesting, or which e-mail he marks as spam, but there is a huge amount of data that has not been marked. As a result, there is an immense need for algorithms that can utilize the small amount of labeled data, combined with the large amount of unlabeled data to build efficient classification systems. Existing semi-supervised classification algorithms may be classified into two categories based on their underlying assumptions. An algorithm is said to satisfy the manifold assumption if it utilizes the fact that the data lie on a low-dimensional manifold in the input space. Usually, the underlying geometry of the data is captured by represent-ing the data as a graph, with samples as the vertices, and the pairwise similarities between the samples as edge weights. Several graph-based algorithms such as Label propagation [3], [4], Markov random walks [5], Graph cut algorithms [6], Spectral graph transducer [7], and Low-density separation [8] proposed in the literature are based on this assumption. Several algorithms have been proposed for semi-super-vised learning which are naturally inductive. Usually, they are based on an assumption, called the cluster assumption [9]. It states that the data samples with high similarity between them must share the same label. This may be equivalently expressed as a condition that the decision boundary between the classes must pass through low-density regions. This assumption allows the unlabeled data to regularize the decision boundary, which, in turn, influences the choice of classification models. Many successful semi-supervised algorithms like TSVM [10] and Semi-supervised SVM [11] follow this approach. These algorithms assume a model for the decision boundary, resulting in an inductive classifier. Manifold regularization [12] is another inductive ap-proach, which is built on the manifold assumption. It attempts to build a maximum-margin classifier on the data, while minimizing the corresponding inconsistency with the similarity matrix. This is achieved by adding a graph-based regularization term to an SVM-based objective function. A related approach called LIAM [13] regularizes the SVM decision boundary using a priori metric information encoded into the Graph Laplacian, and has a fast optimization algorithm. Most semi-supervised learning approaches design spe-cialized learning algorithms to effectively utilize both labeled and unlabeled data. However, it is often the case that a user already has a favorite (well-suited) supervised learning algorithm for his application and would like to improve its performance by utilizing the available unlabeled data. In this light, a more practical approach is to design a technique to utilize the unlabeled samples, regardless of the underlying learning algorithm. Such an approach would accommodate for the task-based selection of a classifier, while providing it with an ability to utilize unlabeled data effectively. We refer to this problem of improving thE performance of any supervised learning algorithm using unlabeled data as Semi-supervised Improvement, to distinguish our work from the standard semi-supervised learning problems.To address the semi-supervised improvement, we propose a boosting framework, termed as SemiBoost, for improving a given supervised learning algorithm with unlabeled data. Similarly to most boosting algorithms [14], SemiBoost improves the classification accuracy iteratively. At each iteration, a number of unlabeled examples will be selected and used to train a new classification model using the given supervised learning algorithm. The trained classification models from each iteration are combined linearly to form a final classification model. An overview of the SemiBoost is presented in Fig. 1. The key difficulties in designing SemiBoost are: 1) how to sample the unlabeled examples for training a new classification model at each iteration and 2) what class labels should be assigned to the selected unlabeled examples. It is important to note that, unlike supervised boosting algorithms, where we select labeled examples that are difficult to classify, SemiBoost needs to select unlabeled examples, at each iteration. One way to address the above questions is to exploit both the clustering assumption and the large margin criterion. One can improve the classification margin by selecting the Manifold regularization [12] is another inductive ap-proach, which is built on the manifold assumption. It attempts to build a maximum-margin classifier on the data, while minimizing the corresponding inconsistency with the similarity matrix. This is achieved by adding a graph-based regularization term to an SVM-based objective function. A related approach called LIAM [13] regularizes the SVM decision boundary using a priori metric information encoded into the Graph Laplacian, and has a fast optimization algorithm. Most semi-supervised learning approaches design spe-cialized learning algorithms to effectively utilize both labeled and unlabeled data. However, it is often the case that a user already has a favorite (well-suited) supervised learning algorithm for his application and would like to improve its performance by utilizing the available unlabeled data. In this light, a more practical approach is to design a technique to utilize the unlabeled samples, regardless of the underlying learning algorithm. Such an approach would accommodate for the task-based selection of a classifier, while providing it with an ability to utilize unlabeled data effectively. We refer to this problem of improving the performance of any supervised learning algorithm using unlabeled data as Semi-supervised Improvement, to distinguish our work from the standard semi-supervised learning problems. To address the semi-supervised improvement, we propose a boosting framework, termed as SemiBoost, for improving a given supervised learning algorithm with unlabeled data. Similarly to most boosting algorithms [14], SemiBoost improves the classification accuracy iteratively. At each iteration, a number of unlabeled examples will be selected and used to train a new classification model using the given supervised learning algorithm. The trained classification models from each iteration are combined linearly to form a final classification model. An overview of the SemiBoost is presented in Fig. 1. The key difficulties in designing SemiBoost are: 1) how to sample the unlabeled examples for training a new classification model at each iteration and 2) what class labels should be assigned to the selected unlabeled examples. It is important to note that, unlike supervised boosting algorithms, where we select labeled examples that are difficult to classify, SemiBoost needs to select unlabeled examples, at each iteration. One way to address the above questions is to exploit both the clustering assumption and the large margin criterion. One can improve the classification margin by selecting the the data used in the algorithm. The second column gives the name of the approach with its reference, followed by a brief description of the method in column 3. Column 4 specifies if the algorithm is naturally inductive (I) or transductive (T). An inductive algorithm can be used to predict the labels of samples that are unseen during training (irrespective of it being labeled or unlabeled); on the other hand, transductive algorithms are limited to predicting only the labels of the unlabeled samples seen during training. Graph-based approaches represent both the labeled and the unlabeled examples by a connected graph in which each example is represented by a vertex and pairs of vertices are connected by an edge if the corresponding examples have large similarity. The well-known approaches in this cate-gory include Harmonic-Function-based approach [18], Spectral Graph Transducer (SGT) [7], Gaussian-process-based approach [23], Manifold Regularization [12], and Label Propagation approach [3], [4]. The optimal class labels for the unlabeled examples are found by minimizing their inconsistency with both the supervised class labels and the graph structure. where L is the combinatorial graph Laplacian. Given a semi-supervised setting, only a few labels in the above consistency measure are assumed to be known and the rest are considered unknown. The task is to assign values to the unknown labels in such a way that the overall inconsistency is minimized. The approach presented in [6] considers the case when yi 2 f 1g, thereby formulating it as a discrete optimization problem and solve it using a minimum-cut approach. Minimum cuts are, however, prone to degenerate solutions and, hence, the objective was minimized using a mixed integer programming approach in [24], which is computationally prohibitive [11]. A continuous relaxation of this objective function where yi 2 ½0; 1& has been considered in several approaches, which is solved using Markov random fields [5], Gaussian random fields, and harmonic functions [18]. The proposed framework is closely related to the graph-based approaches in the sense that it utilizes the pairwise similarities for semi-supervised learning. The inconsistency measure used in the proposed approach follows a similar definition, except that an exponential cost function is used instead of a quadratic cost for violating the labels. Unlike most graph-based approaches, we create a specific classi-fication model by learning from both the labeled and the unlabeled examples. This is particularly important for semi-supervised improvement, whose goal is to improve a given supervised learning algorithm with massive amounts of unlabeled data. The approaches built on cluster assumption utilize the unlabeled data to regularize the decision boundary. In particular, the decision boundary that passes through the region with low density of unlabeled examples is preferred to the one that is densely surrounded with unlabeled examples. These methods specifically extend SVM or related maximum margin classifiers and are not easily extensible to nonmargin-based classifiers like decision trees. Approaches in this category include transductive support vector machine (TSVM) [10], semi-supervised Support Vector Machine (S3VM) [11], and Gaussian processes with null category noise model [23]. The proposed algorithm, on the other hand, is a general approach which allows the choice of a base classifier well-suited to the specific task. Finally, we note that the proposed approach is closely related to the family of ensemble approaches for semi-supervised learning. Ensemble methods have gained sig-nificant popularity under the realm of supervised classifica-tion, with the availability of algorithms such as AdaBoost. A) The semi-supervised counterparts of ensemble algo-rithms rely on the cluster assumption, and prime examples include ASSEMBLE [16] and Semi-supervised MarginBoost (SSMB) [17]. Both of these algorithms work by assigning a pseudolabel to the unlabeled samples and then sampling them for training a new supervised classifier. SSMB and ASSEMBLE are margin-based boosting algorithms which minimize a cost function of the form where H is the ensemble classifier under construction and C is a monotonically decreasing cost function. The term yiHðxiÞ corresponds to the margin definition for labeled samples. A margin definition involves the true label yi, which is not available for the unlabeled samples. A pseudomargin definition is used such as jH ðxi Þj in ASSEMBLE, or HðxiÞ2 in SSMB, thereby getting rid of the yi term in the objective function using the fact that yi 2 f 1g. However, the algorithm relies on the prediction of pseudolabels using the existing ensemble classifier at each iteration. In contrast, the proposed algorithm combines the similarity information along with the classifier predictions to obtain more reliable pseudolabels, which is notably different from the existing approaches. SSMB, on the other hand, requires the base learner to be a semi-supervised algorithm in itself [17], [16]. Therefore, it is solving a different problem of boosting semi-supervised algorithms in contrast with the proposed algorithm. In essence, the SemiBoost algorithm combines the advantages of graph-based and ensemble methods, result-ing in a more general and powerful approach for semi-supervised learning. [17] SEMI-SUPERVISED BOOSTING We first describe the semi-supervised improvement pro-blem formally, and then, present the SemiBoost algorithm. 3.1 Semi-Supervised Improvement Let D ¼ fx1; x2; . . . ; xng denote the entire data set, including both the labeled and the unlabeled examples. Suppose that the first nl examples are labeled, given by where each class label yli is either þ1 or 1. We denote by yu ¼ ðyu1; yu2; . . . ; yunu Þ the imputed class labels of unlabeled examples, where nu ¼ n nl. Let the labels for the entire data set be denoted as y ¼ ½yl; yu&. Let S ¼ ½Si;j&n n denote the symmetric similarity matrix, where Si;j 0 represents the similarity between xi and xj. Furthermore, let Sll denote the nl nl submatrix of the similarity matrix corresponding to the nl labeled exam-ples and Slu denote the nl nu submatrix between labeled and unlabeled samples. The submatrices Suu and Sul can be defined correspondingly. Let A denote the given supervised learning algorithm. The goal of semi-supervised improvement is to improve the performance of A iteratively by treating A like a black box, using the unlabeled examples and the pairwise similarity S. A brief outline of the SemiBoost algorithm for semi-supervised improvement is presented in Fig. 2. It is important to distinguish the problem of semi-supervised improvement from the existing semi-supervised classification approaches. As discussed in Section 2, any ensemble-based algorithm must rely on the pseudolabels for building the next classifier in the ensemble. On the other hand, graph-based algorithms use the pairwise similarities between the samples and assign the labels to unlabeled samples such that they are consistent with the similarity. In the semi-supervised improvement problem, we aim to build an ensemble classifier which utilizes the unlabeled samples in the way a graph-based approach would utilize. SemiBoost To improve the given learning algorithm A, we follow the idea of boosting by running the algorithm A iteratively. A new classification model will be learned at each iteration using the algorithm A, and the learned classification models at different iterations will be linearly combined to form the final classification model. Objective Function The unlabeled samples must be assigned labels following two main criteria: 1) The points with high similarity among unlabeled samples must share the same label and 2) those unlabeled samples which are highly similar to a labeled sample must share its label. Our objective function F ðy; SÞ is a combination of two terms: one measuring the incon-sistency between labeled and unlabeled examples Flðy; SÞ and the other measuring the inconsistency among the unlabeled examples Fuðyu; SÞ. Inspired by the harmonic function approach, we define Fuðy; SÞ, the inconsistency between class any objective functions using similarity or kernel matrices require the kernel to be positive semidefinite to maintain the convexity of the objective function (e.g., SVM). How-ever, since expðxÞ is a convex function,1 and we assume that is nonnegative 8i; j, the function Fu ðyu; SÞ is convex irrespective of the positive definiteness of the similarity matrix. This allows similarity matrices which are asym-metric (e.g., similarity computed using KL-divergence) without changing the convexity of the objective function. Asymmetric similarity matrices arise when using directed graphs for modeling classification problems, and are shown to perform better in certain applications related to text categorization [26]. Though, our approach can work for general similarity matrices, we assume that the similarity matrix provided is symmetric. Note that (1) can be expanded as Our choice of F ðy; SÞ is a mixture of exponential loss functions and is motivated by the traditional exponential loss used in Boosting and the resulting large margin classifier. However, any convex (monotonic) loss function should work with the current framework. labels of the data and, hence, can proceed to increase/ decrease the weights assigned to samples based on the previous iteration. In SemiBoost, we do not have the true class labels for the unlabeled data, which make it challen-ging to estimate the difficulty of classification. However, Proposition 2 gives us the result that selecting the most confident unlabeled data samples is optimal for reducing the objective function. Intuitively, using the samples with highly confident labeling is a good choice because they are consistent with the pairwise similarity information along with their classifications. The values of pi and qi tend to be large if: 1) xi cannot be classified confidently, i.e., jHij is small, and one of its close neighbors is labeled, which corresponds to the first term in (9) and (10), and 2) the example xi is highly similar to some unlabeled examples that are already confidently classified, i.e., large si;j and jHjj for unlabeled example xj. This corresponds to the second term in (9) and (10). This indicates that the similarity information plays an important role in guiding the sample selection, in contrast with the previous approaches like ASSEMBLE and SSMB, where the samples are selected to increase the value of jHij alone. Similar to most boosting algorithms, we can show that the proposed semi-supervised boosting algorithm reduces the original objective function F exponentially. This result is summarized in the following theorem: Theorem 1. Let 1; . . . ; t be the combination weights that are computed by running the SemiBoost algorithm (Fig. 1). Then, the objective function at the ðt þ 1Þst iteration, i.e., Ftþ1, is bounded as follows: labels are correct. This is reasonable given the fact that, since there are very few labeled samples, one can ensure their correctness without much difficulty. Implementation Sampling Sampling forms the most important step for SemiBoost, just like any other boosting algorithm. The criterion for sampling usually considers the following issues: 1) How many samples must be selected from the unlabeled samples available for training and 2) what is the distribution according to which the sampling must be done. Supervised boosting algorithms like AdaBoost have the true labels available, which makes it easy to determine which samples to choose or not to choose. On the other hand, the labels assigned during the SemiBoost iteration are pseudolabels, and may be prone to errors. This suggests that we should choose only the top few most confident data points for SemiBoost. But, selecting a small number of samples might make the convergence slow, and selecting too large a sample might include noninformative or even poor samples into the training set. The choice currently is made empirically; selecting top 10 percent of the samples seem to work well in practice. From Proposition 3, to reduce, it is preferable to select the samples with a large value of hsgdhjwrk any noif maschjs jerueb skwsiwd ejhgthp[p abahsgts gfAT where PsðxiÞ is the probability that the data point xi is sampled from the transduction set. Stopping Criterion According to the optimization procedure, SemiBoost stops when 0, indicating that addition of that classifier would increase the objective function instead of decreasing it. However, the value of decreases very fast in the beginning, and eventually, the rate of decrease falls down, taking a large number of iterations to actually make it negative. We currently use an empirically chosen fixed number of classifiers in the ensemble, specified as a parameter T . We set the value of T ¼ 20. Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves.The process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers learn automatically without human intervention or assistance and adjust actions accordingly.Some machine learning methodsMachine learning algorithms are often categorized as supervised or unsupervised.Supervised machine learning algorithms can apply what has been learned in the past to new data using labeled examples to predict future events. Starting from the analysis of a known training dataset, the learning algorithm produces an inferred function to make predictions about the output values. The system is able to provide targets for any new input after sufficient training. The learning algorithm can also compare its output with the correct, intended output and find errors in order to modify the model accordingly.In contrast, unsupervised machine learning algorithms are used when the information used to train is neither classified nor labeled. Unsupervised learning studies how systems can infer a function to describe a hidden structure from unlabeled data. The system doesn’t figure out the right output, but it explores the data and can draw inferences from datasets to describe hidden structures from unlabeled data.Semi-supervised machine learning algorithms fall somewhere in between supervised and unsupervised learning, since they use both labeled and unlabeled data for training – typically a small amount of labeled data and a large amount of unlabeled data. The systems that use this method are able to considerably improve learning accuracy. Usually, semi-supervised learning is chosen when the acquired labeled data requires skilled and relevant resources in order to train it / learn from it. Otherwise, acquiringunlabeled data generally doesn’t require additional resources.Reinforcement machine learning algorithms is a learning method that interacts with its environment by producing actions and discovers errors or rewards. Trial and error search and delayed reward are the most relevant characteristics of reinforcement learning. This method allows machines and software agents to automatically determine the ideal behavior within a specific context in order to maximize its performance. Simple reward feedback is required for the agent to learn which action is best; this is known as the reinforcement signal.Machine learning enables analysis of massive quantities of data. While it generally delivers faster, more accurate results in order to identify profitable opportunities or dangerous risks, it may also require additional time and resources to train it properly. Combining machine learning with AI and cognitive technologies can make it even more effective in processing large volumes of information.Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to effectively perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model of sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in the applications of email filtering, detection of network intruders, and computer vision, where it is infeasible to develop an algorithm of specific instructions for performing the task. Machine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a field of study within machine learning, and focuses on exploratory data analysis through unsupervised learning.[3][4] In its application across business problems, machine learning is also referred to as predictive analytics.The name machine learning was coined in 1959 by Arthur Samuel. Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E. This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper Computing Machinery and Intelligence, in which the question Can machines think? is replaced with the question Can machines do what we (as thinking entities) can do?.[7] In Turing's proposal the various characteristics that could be possessed by a thinking machine and the various implications in constructing one are exposed.Machine learning tasks are classified into several broad categories. In supervised learning, the algorithm builds a mathematical model from a set of data that contains both the inputs and the desired outputs. For example, if the task were determining whether an image contained a certain object, the training data for a supervised learning algorithm would include images with and without that object (the input), and each image would have a label (the output) designating whether it contained the object. In special cases, the input may be only partially available, or restricted to special feedback.Semi-supervised learning algorithms develop mathematical models from incomplete training data, where a portion of the sample input doesn't have labels.Classification algorithms and regression algorithms are types of supervised learning. Classification algorithms are used when the outputs are restricted to a limited set of values. For a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email. For an algorithm that identifies spam emails, the output would be the prediction of either spam or not spam, represented by the Boolean values true and false. Regression algorithms are named for their continuous outputs, meaning they may have any value within a range. Examples of a continuous value are the temperature, length, or price of an object.In unsupervised learning, the algorithm builds a mathematical model from a set of data which contains only inputs and no desired output labels. Unsupervised learning algorithms are used to find structure in the data, like grouping or clustering of data points. Unsupervised learning can discover patterns in the data, and can group the inputs into categories, as in feature learning. Dimensionality reduction is the process of reducing the number of features, or inputs, in a set of data.Active learning algorithms access the desired outputs (training labels) for a limited set of inputs based on a budget, and optimize the choice of inputs for which it will acquire training labels. When used interactively, these can be presented to a human user for labeling. Reinforcement learning algorithms are given feedback in the form of positive or negative reinforcement in a dynamic environment, and are used in autonomous vehicles or in learning to play a game against a human opponent.[2]:3 Other specialized algorithms in machine learning include topic modeling, where the computer program is given a set of natural language documents and finds other documents that cover similar topics. Machine learning algorithms can be used to find the unobservable probability density function in density estimation problems. Meta learning algorithms learn their own inductive bias based on previous experience. In developmental robotics, robot learning algorithms generate their own sequences of learning experiences, also known as a curriculum, to cumulatively acquire new skills through self-guided exploration and social interaction with humans. These robots use guidance mechanisms such as active learning, maturation, motor synergies, and imitation.Arthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term Machine Learning in 1959 while at IBM[8]. As a scientific endeavour, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed neural networks; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[9] Probabilistic reasoning was also employed, especially in automated medical diagnosis. However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favor.[11] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[10]:708–710; 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as "connectionism", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation. Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.[11] It also benefited from the increasing availability of digitized information, and the ability to distribute it via the Internet.Relation to data mining.Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.Relation to optimization.Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples. Relation to statistics Machine learning and statistics are closely related fields. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics. He also suggested the term data science as a placeholder to call the overall field. Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[14] wherein algorithmic model means more or less the machine learning algorithms like Random forest. A core objective of a learner is to generalize from its experience.[2][16] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfit the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer. In addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.Types of learning algorithms The types of machine learning algorithms differ in their approach, the type of data they input and output, and the type of task or problem that they are intended to solve.Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[18] The data is known as training data, and consists of a set of training examples. Each training example has one or more inputs and a desired output, also known as a supervisory signal. In the case of semi-supervised learning algorithms, some of the training examples are missing the desired output. In the mathematical model, each training example is represented by an array or vector, and the training data by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[19] An optimal function will allow the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task. Supervised learning algorithms include classification and regression.[20] Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.Unsupervised learning algorithms take a set of data that contains only inputs, and find structure in the data, like grouping or clustering of data points. The algorithms therefore learn from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. A central application of unsupervised learning is in the field of density estimation in statistics,[21] though unsupervised learning encompasses other domains involving summarizing and explaining data features.Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In machine learning, the environment is typically represented as a Markov Decision Process (MDP). Many reinforcement learning algorithms use dynamic programmingtechniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP, and are used when exact models are infeasible.[22][23] Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.Processes and techniquesSeveral learning algorithms aim at discovering better representations of the inputs provided during training.[25] Classic examples include principal components analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization[26] and various forms of clustering. Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors.[30] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data. Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions, and is assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately.A popular heuristic method for sparse dictionary learning is the K-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine to which classes a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot. In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.[34]Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions. In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular, unsupervised algorithms) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns. Three broad categories of anomaly detection techniques exist.[37] Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as "normal" and abnormal and involves training a classifier (the key difference to many other statistical classification problems is the inherent unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the model.Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision making.Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of interestingness.[38] This rule-based approach generates new rules as it analyzes more data. The ultimate goal, assuming the set of data is large enough, is to help a machine mimic the human brain’s feature extraction and abstract association capabilities for data that has not been categorized.[39]Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves rules to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets.[41] For example, the rule  found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[42]Inductive logic programming (ILP) is an approach to rule-learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[43][44][45] Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.[46] The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains.[47] The neural network itself is not an algorithm, but rather a framework for many different machine learning algorithms to work together and process complex data inputs.[48] Such systems learn to perform tasks by considering examples, generally without being programmed with any task-specific rules.An ANN is a model based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a signal, from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called edges. Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition. Support vector machines (SVMs), also known as support vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.[50] An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.A Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams. In 2006, the online movie company Netflix held the first Netflix Prize competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[54] Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns ("everything is a recommendation") and they changed their recommendation engine accordingly.[55] In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis.[56] In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[57] In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings, and that it may have revealed previously unrecognized influences between artists.Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.[59][60][61] Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems. In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision.] Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of investment.Machine learning approaches in particular can suffer from different data biases. A machine learning system trained on current customers only may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on man-made data, machine learning is likely to pick up the same constitutional and unconscious biases already present in society. Language models learned from data have been shown to contain human-like biases. Machine learning systems used for criminal risk assessment have been found to be biased against black people. In 2015, Google photos would often tag black people as gorillas, and in 2018 this still was not well resolved, but Google reportedly was still using the workaround to remove all gorilla from the training data, and thus was not able to recognize real gorillas at all.[72] Similar issues with recognizing non-white people have been found in many other systems. In 2016, Microsoft tested a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.[74] Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.[75] Concern for reducing bias in machine learning and propelling its use for human good is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who reminds engineers that There’s nothing artificial about AI...It’s inspired by people, it’s created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.[76]Classification machine learning models can be validated by accuracy estimation techniques like the Holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the N-fold-cross-validation method randomly splits the data in k subsets where the k-1 instances of the data are used to train the model while the kth instance is used to test the predictive ability of the training model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[77]In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the False Positive Rate (FPR) as well as the False Negative Rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The Total Operating Characteristic (TOC) is an effective method to express a model's diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used Receiver Operating Characteristic (ROC) and ROC's associated Area Under the Curve (AUC). Machine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices. For example, using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants. Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.Because language contains biases, machines trained on language corpora will necessarily also learn bias. Other forms of ethical challenges, not related to personal biases, are more seen in health care. There are concerns among health care professionals that these systems might not be designed in the public's interest, but as income generating machines. This is especially true in the United States where there is a perpetual ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes in. There is huge potential for machine learning in health care to provide professionals a great tool to diagnose, medicate, and even plan recovery paths for patients, but this will not happen until the personal biases mentioned previously, and these greed biases are addressed.











 





